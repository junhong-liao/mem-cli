## **Implementing and Comparing Memory Types** ### **Objective** Build a simple conversational agent using [**LangChain**](https://docs.langchain.com) that implements and compares two different memory types: short-term (thread-scoped) and long-term (cross-thread, persistent). The goal is to see how these different memory approaches affect the agent’s ability to handle context and recall information. Memory is a good construct for understanding how agents work because it is easy to understand, but it can be implemented through many different primitives. This exercise is designed to help you use modern abstraction tools like LangChain (though if you feel inclined there are a ton of other abstractions you can use! PydanticAI, LlamaIndex, AI SDK, etc.), deepen your understanding of conversational context, and practice writing clear, maintainable code. ### **Background** Modern AI agents rely on memory to deliver context-aware, adaptive, and efficient user experiences. In LangChain, memory is a first-class concept that enables agents to remember previous interactions, learn from feedback, and adapt to user preferences. This is extremely important for our tool, as our platform is used for many hours every day. As we move across roles & departments, it will be extremely important for our agents to both learn about specific users and learn patterns across users. When designing memory for AI agents, there are several choices to make (e.g., what information is stored, when information is stored, how information is stored). Two of the most important design choices are the distinction between short-term and long-term memory: 1. **Short-term Memory (Thread-Scoped)** Short-term memory tracks the ongoing conversation within a single session or "thread." It maintains message history and other stateful data (such as uploaded files or retrieved documents) for the duration of a conversation. This memory is persisted using thread-scoped checkpoints, allowing conversations to be resumed at any time. However, as conversations grow, the full history may exceed the context window of the underlying LLM, leading to performance and cost challenges. To address this, applications often implement strategies to filter, summarize, or window the conversation history. 1. **Long-term Memory (Cross-Thread, Namespace-Scoped)** Long-term memory stores user-specific or application-level data across sessions and threads. It is organized by custom namespaces (e.g., user or org IDs) and can be recalled in any context, not just within a single conversation. Long-term memory supports various types of information, including: - **Semantic memory:** Facts and knowledge (e.g., user preferences, learned information) - **Episodic memory:** Past experiences or actions (e.g., previous agent actions, user events) - **Procedural memory:** Rules or instructions (e.g., system prompts, agent instructions) LangChain provides flexible storage backends for long-term memory, supporting both profile-based (single document) and collection-based (multiple documents) approaches. Each has trade-offs in terms of recall, update complexity, and context richness. ### **Task Overview** 1. **Review** the [**LangChain memory documentation**](https://docs.langchain.com/oss/python/langgraph/memory). 2. **Set up** a minimal CLI conversational agent with short-term memory (Python, OpenAI or similar LLM). 3. **Implement** at least one strategy for long-term memory described above. 4. **Demonstrate** the differences by running a short scripted conversation (pseudo harness). 5. **Document** your approach and summarize the trade-offs between the memory types you implemented. ### **Requirements** - Agent should handle context-dependent questions (e.g., pronoun resolution, follow-ups). - Provide a README for the harness. - Include a brief write-up. ### **Time Expectation** This project is designed to be completed in **2–4 hours**. ### **Submission** - Please share with us a public GitHub link or a zip file of your code. Either works! - Also record a loom walking us through it! ### Next Steps We’ll spend 30-45 minutes with a member of the team to go over the work & ask you questions. ---
